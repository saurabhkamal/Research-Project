# -*- coding: utf-8 -*-
"""deplot_prompt_baseline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uYIgK5Q-430nu64qJ6Q2slmJ-Jkvup4u
"""

# 1. Install Dependencies
!pip install -q transformers torch accelerate pillow openpyxl tqdm

# 2. Imports and Setup
import os
import re
import json
import random
import time
from datetime import datetime
import pandas as pd
import torch
from tqdm import tqdm
from PIL import Image

from transformers import (
    Blip2Processor,
    Blip2ForConditionalGeneration,
    AutoProcessor,
    Pix2StructForConditionalGeneration
)

from huggingface_hub import notebook_login
from google.colab import drive

# -------------- Mount Drive --------------
drive.mount('/content/drive')

# -------------- Configuration --------------
SEED = 42
random.seed(SEED)
torch.manual_seed(SEED)

HF_TOKEN = "***************************"   # your token
MODEL_ID = "Salesforce/blip2-opt-2.7b"
DTYPE = torch.float16

os.environ["HUGGINGFACE_HUB_TOKEN"] = HF_TOKEN
notebook_login()

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"‚úÖ Running on {device.upper()}")

# -------------- Paths --------------
DATA_ROOT = "/content/drive/MyDrive/Project/test_folder"
TF_JSON = os.path.join(DATA_ROOT, "TF_data.json")
MC_JSON = os.path.join(DATA_ROOT, "MC_data.json")

DETAILED_RESULTS_XLSX = os.path.join(DATA_ROOT, "deplot_detailed_results.xlsx")
SUMMARY_RESULTS_XLSX  = os.path.join(DATA_ROOT, "deplot_summary_result.xlsx")

assert os.path.exists(TF_JSON), f"TF JSON not found: {TF_JSON}"
assert os.path.exists(MC_JSON), f"MC JSON not found: {MC_JSON}"

# DePlot Extraction Helper
# Load DePlot (chart ‚Üí table) once
dep_processor = AutoProcessor.from_pretrained("google/deplot")
dep_model     = Pix2StructForConditionalGeneration.from_pretrained("google/deplot").to(device)

def extract_table_with_deplot(image_path):
    img = Image.open(image_path).convert("RGB")
    inputs = dep_processor(
        images=img,
        text="Generate underlying data table of the figure below:",
        return_tensors="pt"
    ).to(device)
    out = dep_model.generate(**inputs, max_new_tokens=512)
    table_text = dep_processor.decode(out[0], skip_special_tokens=True)
    return table_text.strip()

# This runs DePlot per image (sequentially), which is slow.

# -------------- Helpers --------------
def normalize_tf(pred):
    p = str(pred).strip().lower()
    if re.fullmatch(r"(true|t)", p): return "True"
    if re.fullmatch(r"(false|f)", p): return "False"
    if "true" in p and "false" not in p: return "True"
    if "false" in p and "true" in p: return "False"
    return "True" if "yes" in p else ("False" if "no" in p else "False")

def normalize_mc(pred, choices=None):
    if not pred:
        return ""
    p = str(pred).strip().upper()
    m = re.search(r"\b([A-F])\b", p)
    if m:
        return m.group(1)
    if choices:
        for idx, c in enumerate(choices):
            if c and c.strip().lower() in p.lower():
                return chr(ord("A") + idx)
    return p[:1] if p else ""

def build_prompt_with_table(qtype, question, table_text, choices=None):
    prompt = f"Extracted Table:\n{table_text}\n\n"
    if qtype == "TF":
        prompt += (
            "Example 1: Net profit rose in 2020 vs 2019? <answer>False</answer>\n"
            "Example 2: Expenses decreased in 2022 vs 2021? <answer>True</answer>\n"
            "<start_of_image>\n"
            "You are given a table from a chart and a True/False question.\n"
            "Respond ONLY with 'True' or 'False' inside <answer> tags.\n\n"
            f"Question: {question}\n<answer>"
        )
    elif qtype == "MC":
        example = (
            "Example 1: Which year had highest revenue? Choices: A.2018 B.2019 C.2020 D.2021 <answer>A</answer>\n"
            "Example 2: Which product had lowest cost? Choices: A.P1 B.P2 C.P3 D.P4 <answer>D</answer>\n"
            "Example 3: Which region grew fastest? Choices: A.Asia B.Europe C.US D.Africa <answer>B</answer>\n"
        )
        choices_str = "\n".join(f"{chr(ord('A')+i)}. {c}" for i, c in enumerate(choices or []))
        prompt += (
            "You are given a table from a chart and a multiple‚Äêchoice question.\n"
            "Respond ONLY with the letter inside <answer> tags.\n\n"
            f"Question: {question}\nChoices:\n{choices_str}\n<answer>"
        )
    return prompt


def extract_final_answer(raw_output: str, choices=None):
    if not isinstance(raw_output, str) or not raw_output.strip():
        return ""
    text = raw_output.strip()
    m_tag = re.findall(r"<\s*answer\s*>\s*([A-Z0-9TrueFalse]+?)\s*<\s*/\s*answer\s*>", text,
                       flags=re.IGNORECASE)
    if m_tag:
        ans = m_tag[-1].strip().capitalize()
        return ans
    if choices:
        for idx, c in enumerate(choices):
            if c and c.strip().lower() in text.lower():
                return chr(ord('A') + idx)
    m_fallback = re.findall(r"\b(True|False|[A-F])\b", text, flags=re.IGNORECASE)
    if m_fallback:
        return m_fallback[-1].strip().capitalize()
    return ""


# ---------------- Dataset Loader ----------------
def load_items(path, qtype, subfolder):
    with open(path, "r", encoding="utf-8") as f:
        items = json.load(f)
    return [
        {
            "type":      qtype,
            "question":  it["question"],
            "answer":    it["answer"],
            "choices":   (it.get("choices") if qtype=="MC" else None),
            "image_path": os.path.join(DATA_ROOT, subfolder, it["image"])
        }
        for it in items
    ]

def load_local_dataset(num_mc=10, num_tf=10):
    mc = load_items(MC_JSON, "MC", "MC_images")
    tf = load_items(TF_JSON, "TF", "TF_images")
    data = mc[:num_mc] + tf[:num_tf]
    random.shuffle(data)
    return data

# -------------- Model Setup --------------
print("üîÑ Loading BLIP-2 model ‚Ä¶")
processor = Blip2Processor.from_pretrained(MODEL_ID)
model = Blip2ForConditionalGeneration.from_pretrained(
    MODEL_ID,
    torch_dtype=DTYPE,
    device_map="auto"
).to(device)
model.eval()
print("‚úÖ Model loaded on:", device)

# -------------- Inference Function --------------
@torch.no_grad()
def infer_path_b(image_path, question, qtype, choices=None):
    # 1) extract table
    table_text = extract_table_with_deplot(image_path)

    # 2) build prompt with table
    prompt = build_prompt_with_table(qtype, question, table_text, choices)

    # 3) load & preprocess image
    image = Image.open(image_path).convert("RGB")


    # 4) BLIP-2 inference
    inputs = processor(
        images=image,
        text=prompt,
        return_tensors="pt"
        ).to(device, dtype=DTYPE)

    out_ids = model.generate(**inputs, max_new_tokens=128)

    # 5) deocde
    answer  = processor.decode(out_ids[0], skip_special_tokens=True)

    return answer.strip()

def accuracy(preds, gts):
    return sum(str(p).strip()==str(gt).strip() for p,gt in zip(preds, gts)) / len(preds) if preds else 0

# ---------------- Evaluation ----------------
def evaluate_and_summarize(examples):
    rows, preds, gts, types = [], [], [], []
    print("üöÄ Running inference ‚Ä¶")

    start_all = time.time()

    for ex in tqdm(examples, desc="Evaluating", unit="example"):
        t0 = time.time()
        raw = infer_path_b(ex["image_path"], ex["question"], ex["type"], ex.get("choices"))
        t1 = time.time()
        time_taken = t1 - t0

        if ex["type"] == "MC":
            ans = extract_final_answer(raw, ex.get("choices"))
            norm= normalize_mc(ans, ex.get("choices"))
        else:
            ans = extract_final_answer(raw, None)
            norm= normalize_tf(ans)

        rows.append({
            "image_path":       ex["image_path"],
            "type":             ex["type"],
            "question":         ex["question"],
            "ground_truth":     ex["answer"],
            "prediction_by_model": norm,
            "raw_output":       (raw[:200] + "..." if len(raw)>200 else raw),
            "raw_output_2":     raw,
            "raw_output_length": len(raw),
            "inference_time_secs": time_taken,
            "tokens_per_question": None  # text token count could be added
        })
        preds.append(norm)
        gts.append(ex["answer"])
        types.append(ex["type"])

    total_time = time.time() - start_all
    avg_time = total_time / len(examples) if examples else 0
    throughput = len(examples) / total_time if total_time > 0 else 0

    summary = {
        "model_name":        MODEL_ID,
        "MC_Score":          round(accuracy([p for p,t in zip(preds,types) if t=="MC"],
                                            [g for g,t in zip(gts,types) if t=="MC"]), 4),
        "TF_Score":          round(accuracy([p for p,t in zip(preds,types) if t=="TF"],
                                            [g for g,t in zip(gts,types) if t=="TF"]), 4),
        "Weighted_Avg":      round(accuracy(preds, gts), 4),
        "Total_Questions":   len(preds),
        "Total_Time_secs":   round(total_time, 2),
        "Avg_Time_per_Q":    round(avg_time, 4),
        "Examples_per_sec":  round(throughput, 2),
        "Timestamp":         datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }
    print("‚úÖ Inference completed!")
    print(f"üìä MC Accuracy: {summary['MC_Score']}")
    print(f"üìä TF Accuracy: {summary['TF_Score']}")
    print(f"üìä Weighted Avg: {summary['Weighted_Avg']}")
    print(f"‚è± Total Time (s): {summary['Total_Time_secs']}")
    print(f"üìà Throughput (examples/sec): {summary['Examples_per_sec']}")
    return pd.DataFrame(rows), summary

# ---------------- Main Execution ----------------
if __name__ == "__main__":
    examples = load_local_dataset(num_mc=10, num_tf=10)
    print(f"Total examples: {len(examples)}")
    df_details, summary = evaluate_and_summarize(examples)

    timestamp     = datetime.now().strftime("%Y%m%d_%H%M%S")
    detailed_path = DETAILED_RESULTS_XLSX.replace(".xlsx", f"_{timestamp}.xlsx")
    df_details.to_excel(detailed_path, index=False)
    print("‚úÖ Saved detailed results:", detailed_path)

    df_sum = pd.DataFrame([summary])
    if os.path.exists(SUMMARY_RESULTS_XLSX):
        old   = pd.read_excel(SUMMARY_RESULTS_XLSX)
        df_sum = pd.concat([old, df_sum], ignore_index=True)
    df_sum.to_excel(SUMMARY_RESULTS_XLSX, index=False)
    print("‚úÖ Saved summary results:", SUMMARY_RESULTS_XLSX)



