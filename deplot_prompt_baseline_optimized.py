# -*- coding: utf-8 -*-
"""deplot_prompt_baseline_optimized.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17qN9j3GJJ_Pw4IGZ4DCkSRNCOFhbGYyq
"""

# Install required libraries quietly
!pip install -q transformers torch accelerate pillow openpyxl tqdm

import os
import re
import json
import random
import time
import pickle                   # for caching extracted tables
from datetime import datetime

import pandas as pd            # for saving results to Excel
import torch
from tqdm import tqdm          # progress bars
from PIL import Image          # image loading

# Transformers models + processors
from transformers import (
    Blip2Processor,
    Blip2ForConditionalGeneration,
    AutoProcessor,
    Pix2StructForConditionalGeneration
)

# Hugging Face login (optional)
from huggingface_hub import notebook_login

# Mount Google Drive for data and cache persistence
from google.colab import drive

drive.mount('/content/drive')

# Seed for reproducibility of random operations
SEED = 42
random.seed(SEED)
torch.manual_seed(SEED)

# Hugging Face token, if needed
HF_TOKEN = "************************"
DTYPE = torch.float16

os.environ["HUGGINGFACE_HUB_TOKEN"] = HF_TOKEN
notebook_login()

# Model IDs
BLIP2_ID = "Salesforce/blip2-opt-2.7b"
DEPLOT_ID = "google/deplot"

# Device (GPU if available)
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Running on: {device.upper()}")

# Root folder where your dataset JSON and image folders live
DATA_ROOT = "/content/drive/MyDrive/Project/test_folder"

# JSON files for TF and MC questions
TF_JSON = os.path.join(DATA_ROOT, "TF_data.json")
MC_JSON = os.path.join(DATA_ROOT, "MC_data.json")

# Output file paths
DETAILED_RESULTS_XLSX = os.path.join(DATA_ROOT, "deplot_opt_detailed.xlsx")
SUMMARY_RESULTS_XLSX  = os.path.join(DATA_ROOT, "deplot_opt_summary.xlsx")

# Simple assertions to ensure dataset exists
assert os.path.exists(TF_JSON), f"TF JSON not found: {TF_JSON}"
assert os.path.exists(MC_JSON), f"MC JSON not found: {MC_JSON}"

# Processor for chart‚Üítable model
dep_processor = AutoProcessor.from_pretrained(DEPLOT_ID)

# Load DePlot model on GPU for speed
dep_model = Pix2StructForConditionalGeneration.from_pretrained(
    DEPLOT_ID
).to(device)

# Caching Setup (So we don‚Äôt re-run DePlot per image)

# Path for caching DePlot outputs in Drive
cache_path = "/content/drive/MyDrive/Project/deplot_cache.pkl"

# Load cache if it exists, otherwise initialize empty dict
if os.path.exists(cache_path):
    with open(cache_path, "rb") as f:
        deplot_cache = pickle.load(f)
else:
    deplot_cache = {}

# Batched DePlot Extraction Helper

def extract_tables_with_deplot_batch(image_paths, batch_size=8):
    """
    Runs DePlot in batches for a list of image paths.

    Args:
        image_paths (list[str]): list of chart image file paths.
        batch_size (int): how many images to batch together.

    Returns:
        List[str]: extracted table texts in same order.
    """
    all_tables = []
    for i in range(0, len(image_paths), batch_size):
        batch_paths = image_paths[i : i + batch_size]

        # Load all images in this batch
        batch_imgs = [Image.open(p).convert("RGB") for p in batch_paths]
        # Create repeated text prompts for each image in the batch
        text_prompts = ["Generate underlying data table of the figure below:"] * len(batch_imgs)

        # Preprocess images + text into tensors for DePlot
        inputs = dep_processor(
            images=batch_imgs,
            text=text_prompts,
            return_tensors="pt"
        ).to(device)

        # Generate table tokens from DePlot
        outs = dep_model.generate(**inputs, max_new_tokens=128)

        # Decode all outputs into text
        tables = dep_processor.batch_decode(outs, skip_special_tokens=True)
        tables = [t.strip() for t in tables]

        all_tables.extend(tables)
    return all_tables

# Get or Cache DePlot Output

def get_deplot_table(image_path):
    """
    Returns table text for an image; uses cache when possible.
    """
    # If cached, skip re-generation
    if image_path in deplot_cache:
        return deplot_cache[image_path]

    # Otherwise generate via DePlot
    table_text = extract_tables_with_deplot_batch([image_path], batch_size=1)[0]
    deplot_cache[image_path] = table_text

    # Save updated cache to Drive
    with open(cache_path, "wb") as f:
        pickle.dump(deplot_cache, f)

    return table_text

# Helpers: Normalize Text Outputs

def normalize_tf(pred):
    """
    Normalizes BLIP-2 raw text for TF classification.
    """
    p = str(pred).strip().lower()
    if re.fullmatch(r"(true|t)", p): return "True"
    if re.fullmatch(r"(false|f)", p): return "False"
    if "true" in p and "false" not in p: return "True"
    if "false" in p and "true" not in p: return "False"
    return "True" if "yes" in p else ("False" if "no" in p else "False")

def normalize_mc(pred, choices=None):
    """
    Normalizes BLIP-2 raw text to choose A/B/C/D.
    """
    if not pred:
        return ""
    p = str(pred).strip().upper()
    m = re.search(r"\b([A-F])\b", p)
    if m: return m.group(1)
    if choices:
        for idx, c in enumerate(choices):
            if c and c.strip().lower() in p.lower():
                return chr(ord("A") + idx)
    return p[:1] if p else ""

# Prompt Builder

def build_prompt_with_table(qtype, question, table_text, choices=None):
    """
    Builds BLIP-2 prompt by combining extracted table text + question
    """
    prompt = f"Extracted Table:\n{table_text}\n\n"
    if qtype == "TF":
        prompt += (
            "Example 1: Net profit rose in 2020 vs 2019? <answer>False</answer>\n"
            "Example 2: Expenses decreased in 2022 vs 2021? <answer>True</answer>\n"
            "<start_of_image>\n"
            "You are given a table from a chart and a True/False question.\n"
            "Respond ONLY with 'True' or 'False' inside <answer> tags.\n\n"
            f"Question: {question}\n<answer>"
        )
    elif qtype == "MC":
        example = (
            "Example 1: Which year had highest revenue? Choices: A.2018 B.2019 C.2020 D.2021 <answer>A</answer>\n"
            "Example 2: Which product had lowest cost? Choices: A.P1 B.P2 C.P3 D.P4 <answer>D</answer>\n"
            "Example 3: Which region grew fastest? Choices: A.Asia B.Europe C.US D.Africa <answer>B</answer>\n"
        )
        choices_str = "\n".join(f"{chr(ord('A')+i)}. {c}" for i, c in enumerate(choices or []))
        prompt += (
            "You are given a table from a chart and a multiple-choice question.\n"
            "Respond ONLY with the letter inside <answer> tags.\n\n"
            f"Question: {question}\nChoices:\n{choices_str}\n<answer>"
        )
    return prompt


def extract_final_answer(raw_output: str, choices=None):
    if not isinstance(raw_output, str) or not raw_output.strip():
        return ""
    text = raw_output.strip()
    m_tag = re.findall(r"<\s*answer\s*>\s*([A-Z0-9TrueFalse]+?)\s*<\s*/\s*answer\s*>", text,
                       flags=re.IGNORECASE)
    if m_tag:
        ans = m_tag[-1].strip().capitalize()
        return ans
    if choices:
        for idx, c in enumerate(choices):
            if c and c.strip().lower() in text.lower():
                return chr(ord('A') + idx)
    m_fallback = re.findall(r"\b(True|False|[A-F])\b", text, flags=re.IGNORECASE)
    if m_fallback:
        return m_fallback[-1].strip().capitalize()
    return ""


# ---------------- Dataset Loader ----------------
def load_items(path, qtype, subfolder):
    with open(path, "r", encoding="utf-8") as f:
        items = json.load(f)
    return [
        {
            "type":      qtype,
            "question":  it["question"],
            "answer":    it["answer"],
            "choices":   (it.get("choices") if qtype=="MC" else None),
            "image_path": os.path.join(DATA_ROOT, subfolder, it["image"])
        }
        for it in items
    ]

def load_local_dataset(num_mc=10, num_tf=10):
    mc = load_items(MC_JSON, "MC", "MC_images")
    tf = load_items(TF_JSON, "TF", "TF_images")
    data = mc[:num_mc] + tf[:num_tf]
    random.shuffle(data)
    return data

# Load BLIP-2 (Fast Inference Only)

# Load BLIP-2 processor
processor = Blip2Processor.from_pretrained(BLIP2_ID)

# Load BLIP‚Äê2 model for inference
model = Blip2ForConditionalGeneration.from_pretrained(
    BLIP2_ID,
    torch_dtype=torch.float16,
    device_map="auto"
).to(device)

# Evaluation mode (disable dropout etc.)
model.eval()

# Inference Function (Path B)

@torch.no_grad()
def infer_path_b(image_path, question, qtype, choices=None):
    """
    Runs DePlot caching + BLIP-2 inference for one example.
    """
    # 1) DePlot table from cache or model
    table_text = get_deplot_table(image_path)

    # 2) Build the BLIP-2 prompt
    prompt = build_prompt_with_table(qtype, question, table_text, choices)

    # 3) Load and preprocess chart image
    image = Image.open(image_path).convert("RGB")

    # 4) BLIP-2 inference on image+prompt
    inputs = processor(
        images=image,
        text=prompt,
        return_tensors="pt"
    ).to(device)

    out = model.generate(**inputs, max_new_tokens=128)

    # 5) Decode answer string
    answer = processor.decode(out[0], skip_special_tokens=True)
    return answer.strip()

# Evaluation + Metrics

def accuracy(preds, gts):
    if not preds: return 0
    return sum(str(p).strip()==str(gt).strip() for p,gt in zip(preds, gts)) / len(preds)

def evaluate_and_summarize(examples):
    rows, preds, gts, types = [], [], [], []
    print("üöÄ Running inference ‚Ä¶")

    start_all = time.time()

    for ex in tqdm(examples, desc="Evaluating", unit="example"):
        t0 = time.time()
        raw = infer_path_b(ex["image_path"], ex["question"], ex["type"], ex.get("choices"))
        t1 = time.time()

        time_taken = t1 - t0

        # Normalize to TF/MC label
        if ex["type"]=="MC":
            extracted = extract_final_answer(raw, ex.get("choices"))
            norm = normalize_mc(extracted, ex.get("choices"))
        else:
            extracted = extract_final_answer(raw, None)
            norm = normalize_tf(extracted)

        # Save detailed row
        rows.append({
            "image_path": ex["image_path"],
            "type":       ex["type"],
            "question":   ex["question"],
            "ground_truth": ex["answer"],
            "prediction_by_model": norm,
            "raw_output": raw,
            "inference_time_secs": round(time_taken, 4)
        })
        preds.append(norm)
        gts.append(ex["answer"])
        types.append(ex["type"])

    total_time = time.time() - start_all
    avg_time   = total_time/len(examples) if examples else 0
    throughput = len(examples)/total_time if total_time>0 else 0

    summary = {
        "model_name": BLIP2_ID,
        "MC_Score": round(accuracy([p for p,t in zip(preds,types) if t=="MC"],
                                   [g for g,t in zip(gts,types) if t=="MC"]), 4),
        "TF_Score": round(accuracy([p for p,t in zip(preds,types) if t=="TF"],
                                   [g for g,t in zip(gts,types) if t=="TF"]), 4),
        "Weighted_Avg": round(accuracy(preds, gts), 4),
        "Total_Questions": len(preds),
        "Total_Time_secs": round(total_time, 2),
        "Avg_Time_per_Q": round(avg_time, 4),
        "Examples_per_sec": round(throughput, 2),
        "Timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }

    print("‚úÖ Inference completed!")
    return pd.DataFrame(rows), summary

if __name__ == "__main__":
    examples = load_local_dataset(num_mc=10, num_tf=10)
    print(f"Total examples: {len(examples)}")
    print("üöÄ Running inference ‚Ä¶")

    df_details, summary = evaluate_and_summarize(examples)

    # ---- Print formatted summary like in baseline.ipynb ----
    print("üìä MC Accuracy:", summary["MC_Score"])
    print("üìä TF Accuracy:", summary["TF_Score"])
    print("üìä Weighted Avg:", summary["Weighted_Avg"])
    print("‚è± Total Time (s):", summary["Total_Time_secs"])
    print("üìà Throughput (examples/sec):", summary["Examples_per_sec"])

    # Save detailed Excel
    df_details.to_excel(DETAILED_RESULTS_XLSX, index=False)
    print("‚úÖ Saved detailed results:", DETAILED_RESULTS_XLSX)

    # Append / save summary Excel
    df_sum = pd.DataFrame([summary])
    if os.path.exists(SUMMARY_RESULTS_XLSX):
        old = pd.read_excel(SUMMARY_RESULTS_XLSX)
        df_sum = pd.concat([old, df_sum], ignore_index=True)
    df_sum.to_excel(SUMMARY_RESULTS_XLSX, index=False)
    print("‚úÖ Saved summary results:", SUMMARY_RESULTS_XLSX)

